Using TensorFlow 2 backend.

Set float to float64
Warning: 1000 points required, but 1024 points sampled.
Compiling model...
Building residual neural network...
'build' took 0.093345 s

'compile' took 7.564578 s

Initializing variables...
Training model...

Step      Train loss                                                                          Test loss                                                                           Test metric
0         [7.56e-01, 1.25e-01, 3.81e-01, 6.64e-01, 3.76e-04, 3.78e-01, 2.37e-02, 6.26e-03]    [6.57e-01, 7.76e-02, 4.00e-01, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
1000      [1.10e-02, 2.14e-02, 1.20e-02, 2.20e-02, 3.07e-04, 2.86e-03, 4.61e-03, 4.83e-03]    [1.36e-02, 8.08e-03, 1.14e-02, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
2000      [3.91e-03, 3.90e-03, 2.75e-03, 4.42e-03, 3.66e-04, 1.99e-04, 1.50e-03, 1.85e-03]    [8.00e-03, 4.13e-03, 3.77e-03, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
3000      [1.94e-03, 1.83e-03, 1.11e-03, 1.01e-03, 2.89e-04, 9.34e-05, 5.91e-04, 1.05e-03]    [4.07e-03, 2.28e-03, 1.73e-03, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
4000      [9.25e-04, 1.12e-03, 5.36e-04, 3.65e-04, 1.99e-04, 3.59e-05, 2.02e-04, 7.27e-04]    [1.67e-03, 1.25e-03, 7.93e-04, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
5000      [5.28e-04, 7.90e-04, 2.88e-04, 1.59e-04, 1.43e-04, 1.53e-05, 5.95e-05, 4.73e-04]    [8.17e-04, 9.21e-04, 3.74e-04, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
6000      [3.18e-04, 5.69e-04, 1.72e-04, 7.60e-05, 9.88e-05, 1.09e-05, 2.81e-05, 2.75e-04]    [4.98e-04, 7.05e-04, 2.03e-04, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
7000      [1.85e-04, 3.85e-04, 1.05e-04, 3.87e-05, 6.26e-05, 8.67e-06, 1.64e-05, 1.38e-04]    [3.14e-04, 4.92e-04, 1.27e-04, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
8000      [1.12e-04, 2.31e-04, 6.25e-05, 1.80e-05, 3.62e-05, 8.02e-06, 8.55e-06, 6.15e-05]    [1.93e-04, 3.06e-04, 9.22e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
9000      [7.31e-05, 1.29e-04, 3.77e-05, 6.24e-06, 2.13e-05, 7.44e-06, 4.50e-06, 2.62e-05]    [1.13e-04, 1.75e-04, 7.23e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
10000     [5.09e-05, 7.46e-05, 2.49e-05, 1.93e-06, 1.37e-05, 5.40e-06, 2.57e-06, 1.12e-05]    [6.81e-05, 1.01e-04, 5.59e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
11000     [3.58e-05, 4.43e-05, 1.78e-05, 8.98e-07, 9.28e-06, 3.05e-06, 1.44e-06, 4.82e-06]    [4.49e-05, 6.03e-05, 4.15e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
12000     [2.46e-05, 2.74e-05, 1.29e-05, 9.81e-07, 6.60e-06, 1.63e-06, 8.37e-07, 2.14e-06]    [3.13e-05, 3.92e-05, 3.00e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
13000     [1.74e-05, 1.81e-05, 9.20e-06, 9.68e-07, 4.88e-06, 9.67e-07, 6.18e-07, 9.85e-07]    [2.35e-05, 2.81e-05, 2.13e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
14000     [1.28e-05, 1.27e-05, 6.66e-06, 7.93e-07, 3.70e-06, 6.35e-07, 5.24e-07, 5.17e-07]    [1.86e-05, 2.14e-05, 1.54e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
15000     [9.83e-06, 9.36e-06, 5.09e-06, 8.06e-07, 2.90e-06, 4.70e-07, 4.62e-07, 3.65e-07]    [1.53e-05, 1.73e-05, 1.16e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
16000     [7.65e-06, 7.07e-06, 3.90e-06, 6.15e-07, 2.37e-06, 3.39e-07, 3.87e-07, 3.25e-07]    [1.28e-05, 1.38e-05, 8.99e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
17000     [6.15e-06, 5.53e-06, 3.12e-06, 5.90e-07, 2.01e-06, 2.69e-07, 3.28e-07, 3.24e-07]    [1.09e-05, 1.15e-05, 7.27e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
18000     [5.06e-06, 4.50e-06, 2.61e-06, 5.72e-07, 1.76e-06, 2.21e-07, 2.81e-07, 3.16e-07]    [9.29e-06, 9.80e-06, 6.08e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
19000     [4.24e-06, 3.76e-06, 2.24e-06, 5.50e-07, 1.58e-06, 1.89e-07, 2.43e-07, 3.04e-07]    [8.00e-06, 8.43e-06, 5.21e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
20000     [3.64e-06, 3.20e-06, 1.95e-06, 5.24e-07, 1.44e-06, 1.67e-07, 2.13e-07, 2.90e-07]    [7.01e-06, 7.35e-06, 4.55e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
21000     [3.32e-06, 2.80e-06, 1.74e-06, 5.09e-07, 1.33e-06, 1.52e-07, 1.88e-07, 2.95e-07]    [6.38e-06, 6.54e-06, 4.04e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
22000     [2.83e-06, 2.47e-06, 1.59e-06, 4.69e-07, 1.24e-06, 1.39e-07, 1.69e-07, 2.58e-07]    [5.52e-06, 5.79e-06, 3.66e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
23000     [2.52e-06, 2.22e-06, 1.47e-06, 4.42e-07, 1.15e-06, 1.30e-07, 1.52e-07, 2.39e-07]    [4.91e-06, 5.20e-06, 3.36e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00]    []  
Epoch 23000: early stopping

Best model at step 23000:
  train loss: 8.33e-06
  test loss: 1.35e-05
  test metric: []

'train' took 586.530572 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
